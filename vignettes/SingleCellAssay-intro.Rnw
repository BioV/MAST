%\VignetteIndexEntry{SingleCellAssay-intro}
%\VignetteEngine{knitr}
\documentclass{article}
\usepackage{url, graphicx}
\usepackage{color}
\usepackage[cm]{fullpage}
\usepackage[usenames,dvipsnames]{xcolor}
%\usepackage[authoryear]{natbib}

%\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% \VignetteIndexEntry{An Introduction to SingleCellAssay}

%\makeatother
\newcommand{\future}[1]{TODO: {\color{gray} #1}}
\newcommand{\sca}{\texttt{SingleCellAssay}}
\input{symbols.tex}
\begin{document}
\title{An Introduction to SingleCellAssay}


\author{Andrew McDavid and Greg Finak}

\maketitle
\section{Philosophy}
 \sca is an R/Bioconductor package for managing and analyzing Fluidigm single--cell gene expression
 data as well as data from other types of single--cell assays. 
Our goal is to support assays that have multiple \emph{features} (genes,
markers, etc) per \emph{well} (cell, etc) in a flexible manner.
Assays are assumed to be  mostly \emph{complete} in the sense that most \emph{wells}
contain measurements for all features.

\subsection{Internals}
A \texttt{SingleCellAssay} object can be manipulated as a matrix, with rows giving wells and columns giving features.

\subsection{Statistical Testing}
Apart from reading and storing single--cell assay data, the package also
provides functionality for significance testing of differential expression
using a combined binomial and normal--theory likelihood ratio test, as well as
filtering of individual outlier wells. These methods are described our papers. 
% Add citations


\section{Examples}

With the cursory background out of the way, we'll proceed with some examples
to help understand how the package is used.

\subsection{Reading Data}
Data can be imported in a Fluidigm instrument-specific format (the details of
which are undocumented, and likely subject-to-change) or some derived,
annotated format,  or in ``long'' (melted) format, in which each row is a
measurement, so if there are $N$ wells and $M$ cells, then the
\texttt{data.frame} should contain $N \times M$ rows.
The use of key--value mappings makes the reading of various input formats very
flexible, provided that they contain the minimal required information expected
by the package.

For example, the following data set was provided in as a comma-separated value file.
It has the cycle threshold ($\ct$) recorded. 
Non-detected genes are recorded as NAs.
For the Fluidigm/qPCR single cell expression functions to work as expected, we
must use the \emph{expression threshold}, defined as $E_t = c_{\mbox{max}} - \ct$, which is proportional to the log-expression.

Below, we load the package and the data, then compute the expression threshold from the $\ct$, and construct a \texttt{FluidigmAssay}.
<<long-example,warning=FALSE>>=
library(SingleCellAssay)
require(plyr)
data(vbeta)
colnames(vbeta)
vbeta <- computeEtFromCt(vbeta)
vbeta.fa <- FluidigmAssay(vbeta, idvars=c("Subject.ID", "Chip.Number", "Well"), primerid='Gene', measurement='Et', ncells='Number.of.Cells', geneid="Gene",  cellvars=c('Number.of.Cells', 'Population'), phenovars=c('Stim.Condition','Time'), id='vbeta all')
show(vbeta.fa)
@

We see that the variable \texttt{vbeta} is a \texttt{data.frame} from which we
construct the \texttt{FluidigmAssay} object. 
The \texttt{idvars} is the set of column(s) in \texttt{vbeta} that uniquely
identify a well (globally), the \texttt{primerid} is a column(s) that specify the feature measured at this well.
The \texttt{measurement} gives the column name containing the log-expression
measurement, \texttt{ncells} contains the number of cells (or other
normalizing factor) for the well.
\texttt{geneid}, \texttt{cellvars}, \texttt{phenovars} all specify additional
columns to be included in the \texttt{featureData}, \texttt{phenoData}  and
\texttt{cellData} (\future{wellData}). The output is a \texttt{FluidigmAssay}
object with \Sexpr{nrow(cData(vbeta.fa))} wells and \Sexpr{nrow(fData(vbeta.fa))} features. 


We can access the feature--level metadata and the cell--level metadata using
the \texttt{fData} and \texttt{cData} accessors.

<<examineMeta>>=
head(fData(vbeta.fa),3)
head(cData(vbeta.fa),3)
@ 

We see this gives us the set of genes measured in the assay, or the cell-level
metadata (i.e. the number of cells measured in the well, the population this
cell belongs to, the subject it came from, the chip it was run on, the well
id, the stimulation it was subjected to, and the timepoint for the experiment
this cell was part of). The wellKey is a hash of idvars columns, helping to
ensure consistency when splitting and merging \sca objects. 
\future{Some of this ``cell--level'' information could
  arguably be part of the \texttt{@phenoData} slot of the object. This
  functionality is forthcoming but doesn't limit what can be done with the
  package at this stage}.

\subsection{Subsetting, splitting, combining}
It's possible to subset \sca objects by wells and features.
Square brackets (``['') will index on
the first index and by features on the second index. 
Integer and boolean and indices may be used, as well as character vectors
naming the cellKey or the feature (via the primerid).
There is also a \texttt{subset} method, which will evaluate its argument in the frame of the \texttt{cData}, hence will subset by wells.
<<subsets,warning=FALSE>>=
sub1 <- vbeta.fa[1:10,]
show(sub1)
sub2 <- subset(vbeta.fa, Well=='A01')
show(sub2)
sub3 <- vbeta.fa[1:10,6:10]
show(sub3)
cellData(sub3)
featureData(sub3)
@
The cellData and featureData \texttt{AnnotatedDataFrames} are subset
accordingly as well.

A \sca may be split into a list of \sca, which is known as an
\texttt{SCASet}. The split method takes an argument which names the column
(factor) on which to split the data. Each level of the factor will be placed
in its own \sca within the SCASet.
<<split,warning=FALSE>>=
sp1 <- split(vbeta.fa, 'Subject.ID')
show(sp1)
@
The splitting variable can either be a character vector naming column(s) of the \sca, or may be a \texttt{factor} or \texttt{list} of \texttt{factor}s.

It's possible to combine \sca objects or an \texttt{SCASet} with the \texttt{combine} method.
<<combine,warning=FALSE>>=
unloadNamespace("gplots")
combine(x=sp1[[1]],y=sp1[[2]])
combine(sp1)
@ 

\subsection{Filtering}
We can filter and perform some significance tests on the \sca.
We may want to filter any wells with at least two outlier cells where the discrete and continuous parts of the signal are at least 9 standard deviations from the mean. This is a very conservative filtering criteria. We'll group the filtering by the number of cells.

We'll split the assay by the number of cells and look at the concordance plot after filtering. 
<<splitbyncells,warning=FALSE>>=
vbeta.split<-split(vbeta.fa,"Number.of.Cells")
#see default parameters for plotSCAConcordance
plotSCAConcordance(vbeta.split[[1]],vbeta.split[[2]],filterCriteria=list(nOutlier = 1, sigmaContinuous = 9,sigmaProportion = 9))
@

The filtering function has several other options, including whether the filter shuld be applied (thus returning a new SingleCellAssay object) or returned as a matrix of boolean values.

<<otherFiltering, warning=FALSE>>=
vbeta.fa
## Split by 'ncells', apply to each component, then recombine
vbeta.filtered <- filter(vbeta.fa, groups='ncells')
## Returned as boolean matrix
was.filtered <- filter(vbeta.fa, apply_filter=FALSE)
## Wells filtered for being discrete outliers
head(subset(was.filtered, pctout))
@

There's also some functionality for visualizing the filtering.

<<burdenOfFiltering, warning=FALSE, fig.width=4, fig.height=4>>=
burdenOfFiltering(vbeta.fa, 'ncells', byGroup=TRUE)
@

\subsection{Significance testing under the Hurdle model}
There are two frameworks available in the package.  The first framework \texttt{zlm} offers a full linear model to allow arbitrary comparisons and adjustment for covariates. The second framework \texttt{LRT} can be considered essentially performing t-tests (respecting the discrete/continuous nature of the data) between pairs of groups.  \texttt{LRT} is subsumed by the first framework, but might be simpler for some users, so has been kept in the package.

We'll describe \texttt{zlm}.  Models are specified in terms of the variable used as the measure and covariates present in the \texttt{cellData} using symbolic notation, just as the \texttt{lm} function in R.
<<zlmArgs>>=
vbeta.1 <- subset(vbeta.fa, ncells==1)
## Consider the first 20 genes
vbeta.1 <- vbeta.1[,1:20] 
layername(vbeta.1)
head(cData(vbeta.1))
@
Now, for each gene, we can regress on \texttt{Et} the factors \texttt{Population} and \texttt{Subject.ID}.

In each gene, we'll test if the factor \texttt{Population} explains a significant amount of the variation by fitting models with and without \texttt{Population} as an explanatory factor.  An array of genes, metrics and test types is returned.
We'll plot the -log10 P values by gene and test type.
<<zlmExample, warning=FALSE>>=
library(ggplot2)
library(reshape)
zlm.output <- zlm.SingleCellAssay(Et ~ Population + Subject.ID, vbeta.1, hypo.terms='Population', type='LRT')
dimnames(zlm.output)
pvalue <- ggplot(melt(zlm.output[,'Pr(>Chisq)',]), aes(x=primerid, y=-log10(value)))+geom_bar(stat='identity')+facet_wrap(~test.type) + coord_flip()
print(pvalue)
@

In fact, the \text{zlm} framework is quite general, and allows any modeling function that accepts \text{glm}-like arguments to be used, including mixed models such as those available in \texttt{lme4}.

<<lmerExample, warning=FALSE>>=
library(lme4)
lmer.output <- zlm.SingleCellAssay(Et ~ Population +(1|Subject.ID), vbeta.1, lm.fun=glmer,hypo.terms='Population', type='Wald')

@

It is possible to save the model fits so that coefficients can be examined.  See \texttt{?zlm.SingleCellAssay}.
\section{Implementation Details}
Here we provide some background on the implementation of the package. 

There are several fundamental new object types provided by the package.
The \texttt{SCA} is a virtual class that is inherited by
\texttt{SingleCellAssay} and \texttt{FluidigmAssay}. 
The latter also inherits
from \texttt{SingleCellAssay}. 
New types of single cell assays can be incorportated by extending
\texttt{SingleCellAssay} or \texttt{SCA}, as appropriate (depending on the
structure of the data). 

The correspondence between data files and the internal representation in the
package for \texttt{features}, \texttt{cells} or \texttt{wells},
and \texttt{phenotypic data} is provided by the \texttt{Mapping} class which
simply provides a mapping of internal keys (i.e. \texttt{primerid, idvars,
  featurevars, cellvars, phenovars} ) and values (one or more column names in
the assay data files).
These key--value maps allow the constructors to correctly map feature--level,
cell--level, and sample--level information to each measured cell. 
The

The \texttt{Mapping} class makes this very flexible and amenable to many non--standard
tabular data representations, and indepdendent of whatever names a user may
have given different variables in the data set. 

Some mappings are required to construct a valid \texttt{SingleCellAssay} or
\texttt{FluidigmAssay} object.
The required mappings and default maps are present for each single--cell assay
class in the package (i.e. \texttt{SingleCellAssay:::FluidigmMap,
  SingleCellAssay:::FluidigmMapNames, SingleCellAssay:::SingleCellAssayMap},
  and \\\texttt{SingleCellAssay:::SingleCellAssayMapNames}). 

Sets of single cell assays are stored in the \texttt{SCASet} class. A
constructor for SCASet is provided to construct an SCASet directly from a data
frame. 
Alternatively, a SingleCellAssay or derived class can be \texttt{split}
on an arbitray variable 
to produce an SCASet.

\subsection{Extending SingleCellAssay Classes}
Any extension of the package to new types of single--cell assays should implement a new \texttt{class} for the
assay, provide a \texttt{constructor} with the same name as the class, and
provide a set of default required \texttt{map names} and an empty
\texttt{Mapping}, which would then be filled in by the constructor. 

The function \texttt{SingleCellAssayValidity} function can be used as a validity
checking method for classes inheriting from \texttt{SingleCellAssay}.
It will check that the required map names are provided, and that their values
are present in the data. 

On construction of a \texttt{SingleCellAssay} object, the package tests for
completeness, and will fill in the missing data (with NA) if it is not, so
assays with lots of missing data can make reading marginally slower. 

\subsection{Internals}
Internally (within a \texttt{SingleCellAssay}, we store everything as a
\texttt{data.frame} with names of special columns kept in the
\texttt{@mapping} slot that contains a \texttt{Mapping} object.  
The data is stored in long-melted format, in feature-major order, so while not
especially fast or space-efficient, it is rather intended to be very flexible.

Each well, feature \future{, and unit (phenotype)} has various measured covariates.
These are kept in \texttt{AnnotatedDataframes} in slots in the object, which are generated from the base \texttt{data.frame}, if provided.
\future{If not provided, then they can be added after object creation.}

\end{document}
